{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1GdxGO72g8y"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import base64\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as TF\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from torch.cuda import amp\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from torchmetrics import MeanMetric\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from torch.cuda import memory_allocated,empty_cache,memory_cached,reset_max_memory_allocated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MEs_xCA8YwA"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsx9WPOl622H"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
        "        super().__init__()\n",
        "\n",
        "        half_dim = time_emb_dims // 2\n",
        "\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "\n",
        "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
        "\n",
        "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "\n",
        "        self.time_blocks = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb),\n",
        "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
        "        )\n",
        "\n",
        "    def forward(self, time):\n",
        "        return self.time_blocks(time)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels=64):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
        "        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, _, H, W = x.shape\n",
        "        h = self.group_norm(x)\n",
        "        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)  # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n",
        "        h, _ = self.mhsa(h, h, h)  # [B, H*W, C]\n",
        "        h = h.swapaxes(2, 1).view(B, self.channels, H, W)  # [B, C, H*W] --> [B, C, H, W]\n",
        "        return x + h\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels, dropout_rate=0.1, time_emb_dims=512, apply_attention=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.act_fn = nn.SiLU()\n",
        "        # Group 1\n",
        "        self.normlize_1 = nn.GroupNorm(num_groups=8, num_channels=self.in_channels)\n",
        "        self.conv_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "\n",
        "        # Group 2 time embedding\n",
        "        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n",
        "\n",
        "        # Group 3\n",
        "        self.normlize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n",
        "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            self.match_input = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1)\n",
        "        else:\n",
        "            self.match_input = nn.Identity()\n",
        "\n",
        "        if apply_attention:\n",
        "            self.attention = AttentionBlock(channels=self.out_channels)\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # group 1\n",
        "        h = self.act_fn(self.normlize_1(x))\n",
        "        h = self.conv_1(h)\n",
        "\n",
        "        # group 2\n",
        "        # add in timestep embedding\n",
        "        h += self.dense_1(self.act_fn(t))[:, :, None, None]\n",
        "\n",
        "        # group 3\n",
        "        h = self.act_fn(self.normlize_2(h))\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv_2(h)\n",
        "\n",
        "        # Residual and attention\n",
        "        h = h + self.match_input(x)\n",
        "        h = self.attention(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.downsample = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, *args):\n",
        "        return self.downsample(x)\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, *args):\n",
        "        return self.upsample(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels=3,\n",
        "        output_channels=3,\n",
        "        num_res_blocks=2,\n",
        "        base_channels=128,\n",
        "        base_channels_multiples=(1, 2, 4, 8),\n",
        "        apply_attention=(False, False, True, False),\n",
        "        dropout_rate=0.1,\n",
        "        time_multiple=4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        time_emb_dims_exp = base_channels * time_multiple\n",
        "        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels, time_emb_dims_exp=time_emb_dims_exp)\n",
        "\n",
        "        self.first = nn.Conv2d(in_channels=input_channels, out_channels=base_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "\n",
        "        num_resolutions = len(base_channels_multiples)\n",
        "\n",
        "        # Encoder part of the UNet. Dimension reduction.\n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        curr_channels = [base_channels]\n",
        "        in_channels = base_channels\n",
        "\n",
        "        for level in range(num_resolutions):\n",
        "            out_channels = base_channels * base_channels_multiples[level]\n",
        "\n",
        "            for _ in range(num_res_blocks):\n",
        "\n",
        "                block = ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    dropout_rate=dropout_rate,\n",
        "                    time_emb_dims=time_emb_dims_exp,\n",
        "                    apply_attention=apply_attention[level],\n",
        "                )\n",
        "                self.encoder_blocks.append(block)\n",
        "\n",
        "                in_channels = out_channels\n",
        "                curr_channels.append(in_channels)\n",
        "\n",
        "            if level != (num_resolutions - 1):\n",
        "                self.encoder_blocks.append(DownSample(channels=in_channels))\n",
        "                curr_channels.append(in_channels)\n",
        "\n",
        "        # Bottleneck in between\n",
        "        self.bottleneck_blocks = nn.ModuleList(\n",
        "            (\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    dropout_rate=dropout_rate,\n",
        "                    time_emb_dims=time_emb_dims_exp,\n",
        "                    apply_attention=True,\n",
        "                ),\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    dropout_rate=dropout_rate,\n",
        "                    time_emb_dims=time_emb_dims_exp,\n",
        "                    apply_attention=False,\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Decoder part of the UNet. Dimension restoration with skip-connections.\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "\n",
        "        for level in reversed(range(num_resolutions)):\n",
        "            out_channels = base_channels * base_channels_multiples[level]\n",
        "\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                encoder_in_channels = curr_channels.pop()\n",
        "                block = ResnetBlock(\n",
        "                    in_channels=encoder_in_channels + in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    dropout_rate=dropout_rate,\n",
        "                    time_emb_dims=time_emb_dims_exp,\n",
        "                    apply_attention=apply_attention[level],\n",
        "                )\n",
        "\n",
        "                in_channels = out_channels\n",
        "                self.decoder_blocks.append(block)\n",
        "\n",
        "            if level != 0:\n",
        "                self.decoder_blocks.append(UpSample(in_channels))\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups=8, num_channels=in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "\n",
        "        time_emb = self.time_embeddings(t)\n",
        "\n",
        "        h = self.first(x)\n",
        "        outs = [h]\n",
        "\n",
        "        for layer in self.encoder_blocks:\n",
        "            h = layer(h, time_emb)\n",
        "            outs.append(h)\n",
        "\n",
        "        for layer in self.bottleneck_blocks:\n",
        "            h = layer(h, time_emb)\n",
        "\n",
        "        for layer in self.decoder_blocks:\n",
        "            if isinstance(layer, ResnetBlock):\n",
        "                out = outs.pop()\n",
        "                h = torch.cat([h, out], dim=1)\n",
        "            h = layer(h, time_emb)\n",
        "\n",
        "        h = self.final(h)\n",
        "\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lptWrVXb8fNS"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrGoPgLA68xz"
      },
      "outputs": [],
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader:\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "def get_default_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def save_images(images, path, **kwargs):\n",
        "    grid = make_grid(images, **kwargs)\n",
        "    ndarr = grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(path)\n",
        "\n",
        "def get(element: torch.Tensor, t: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Get value at index position \"t\" in \"element\" and\n",
        "        reshape it to have the same dimension as a batch of images.\n",
        "    \"\"\"\n",
        "    ele = element.gather(-1, t)\n",
        "    return ele.reshape(-1, 1, 1, 1)\n",
        "\n",
        "def setup_log_directory(config):\n",
        "    '''Log and Model checkpoint directory Setup'''\n",
        "\n",
        "    if os.path.isdir(config.root_log_dir):\n",
        "        # Get all folders numbers in the root_log_dir\n",
        "        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(config.root_log_dir)]\n",
        "\n",
        "        # Find the latest version number present in the log_dir\n",
        "        last_version_number = max(folder_numbers)\n",
        "\n",
        "        # New version name\n",
        "        version_name = f\"version_{last_version_number + 1}\"\n",
        "\n",
        "    else:\n",
        "        version_name = config.log_dir\n",
        "\n",
        "    # Update the training config default directory\n",
        "    log_dir        = os.path.join(config.root_log_dir,        version_name)\n",
        "    checkpoint_dir = os.path.join(config.root_checkpoint_dir, version_name)\n",
        "\n",
        "    # Create new directory for saving new experiment version\n",
        "    os.makedirs(log_dir,        exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Logging at: {log_dir}\")\n",
        "    print(f\"Model Checkpoint at: {checkpoint_dir}\")\n",
        "\n",
        "    return log_dir, checkpoint_dir\n",
        "\n",
        "def frames2vid(images, save_path):\n",
        "\n",
        "    WIDTH = images[0].shape[1]\n",
        "    HEIGHT = images[0].shape[0]\n",
        "\n",
        "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "#     fourcc = 0\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video = cv2.VideoWriter(save_path, fourcc, 25, (WIDTH, HEIGHT))\n",
        "\n",
        "    # Appending the images to the video one by one\n",
        "    for image in images:\n",
        "        video.write(image)\n",
        "\n",
        "    # Deallocating memories taken for window creation\n",
        "    # cv2.destroyAllWindows()\n",
        "    video.release()\n",
        "    return\n",
        "\n",
        "def display_gif(gif_path):\n",
        "    b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
        "    display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z179fsuMBpRK"
      },
      "source": [
        "# All Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmx3HlZ-8qZL"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class BaseConfig:\n",
        "    DEVICE = get_default_device()\n",
        "    DATASET = \"APTOS\"\n",
        "\n",
        "    # For logging inferece images and saving checkpoints.\n",
        "    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Inference\")\n",
        "    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"checkpoints\")\n",
        "\n",
        "    # Current log and checkpoint directory.\n",
        "    log_dir = \"version_0\"\n",
        "    checkpoint_dir = \"version_0\"\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    TIMESTEPS = 1000 # Define number of diffusion timesteps\n",
        "    IMG_SHAPE = (3, 192, 192)\n",
        "    NUM_EPOCHS = 700\n",
        "    BATCH_SIZE = 1\n",
        "    LR = 2e-5\n",
        "    NUM_WORKERS = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nQqB3jTB9CN"
      },
      "source": [
        "# Load Dataset & Build Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ksESxCA_cDK"
      },
      "outputs": [],
      "source": [
        "def get_dataset(dataset_name='APTOS'):\n",
        "    transforms = TF.Compose(\n",
        "        [\n",
        "            TF.ToTensor(),\n",
        "            TF.Resize((192,192),\n",
        "                      interpolation=TF.InterpolationMode.BICUBIC,\n",
        "                      antialias=True),\n",
        "            TF.RandomHorizontalFlip(),\n",
        "            TF.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if dataset_name == \"APTOS\":\n",
        "        # replace path to input images\n",
        "        dataset = datasets.ImageFolder(root=\"/path\", transform=transforms)\n",
        "    return dataset\n",
        "\n",
        "def get_dataloader(dataset_name='APTOS',\n",
        "                   batch_size=32,\n",
        "                   pin_memory=False,\n",
        "                   shuffle=True,\n",
        "                   num_workers=0,\n",
        "                   device=\"cpu\"\n",
        "                  ):\n",
        "    dataset    = get_dataset(dataset_name=dataset_name)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                            pin_memory=pin_memory,\n",
        "                            num_workers=num_workers,\n",
        "                            shuffle=shuffle\n",
        "                           )\n",
        "    device_dataloader = DeviceDataLoader(dataloader, device)\n",
        "    return device_dataloader\n",
        "\n",
        "def inverse_transform(tensors):\n",
        "    \"\"\"Convert tensors from [-1., 1.] to [0., 255.]\"\"\"\n",
        "    return ((tensors.clamp(-1, 1) + 1.0) / 2.0) * 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC6SZ8-EErma"
      },
      "source": [
        "# Visualize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFSK20WEFz2J"
      },
      "outputs": [],
      "source": [
        "loader = get_dataloader(\n",
        "    dataset_name=BaseConfig.DATASET,\n",
        "    batch_size=128,\n",
        "    device='cpu',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4fj9h8__F2xn",
        "outputId": "81391beb-a7c2-4207-b29d-5f19e43aa38d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20), facecolor='white')\n",
        "\n",
        "for b_image, _ in loader:\n",
        "    b_image = inverse_transform(b_image).cpu()\n",
        "    grid_img = make_grid(b_image / 255.0, nrow=10, padding=True, pad_value=1, normalize=True)\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.axis(\"off\")\n",
        "    print(b_image.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzX2mtQPFD2s"
      },
      "source": [
        "# Diffusion Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w0NQeL06F-Eq"
      },
      "outputs": [],
      "source": [
        "class SimpleDiffusion:\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_diffusion_timesteps=1000,\n",
        "        img_shape=(3, 64, 64),\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        self.num_diffusion_timesteps = num_diffusion_timesteps\n",
        "        self.img_shape = img_shape\n",
        "        self.device = device\n",
        "\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        self.beta  = self.get_betas() # BETAs & ALPHAs required at different places in the Algorithm.\n",
        "\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        self_sqrt_beta                       = torch.sqrt(self.beta)\n",
        "        self.alpha_cumulative                = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_cumulative           = torch.sqrt(self.alpha_cumulative)\n",
        "        self.one_by_sqrt_alpha               = 1. / torch.sqrt(self.alpha)\n",
        "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative)\n",
        "\n",
        "    def get_betas(self):\n",
        "        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n",
        "        scale = 1000 / self.num_diffusion_timesteps\n",
        "        beta_start = scale * 1e-4\n",
        "        beta_end = scale * 0.02\n",
        "        return torch.linspace(\n",
        "            beta_start,\n",
        "            beta_end,\n",
        "            self.num_diffusion_timesteps,\n",
        "            dtype=torch.float32,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "def forward_diffusion(sd: SimpleDiffusion, x0: torch.Tensor, timesteps: torch.Tensor):\n",
        "    eps = torch.randn_like(x0)  # Noise\n",
        "    mean    = get(sd.sqrt_alpha_cumulative, t=timesteps) * x0  # Image scaled\n",
        "    std_dev = get(sd.sqrt_one_minus_alpha_cumulative, t=timesteps) # Noise scaled\n",
        "    sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
        "\n",
        "    return sample, eps "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFX6fzAvFNt3"
      },
      "source": [
        "# Sample Forward Diffusion Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8aKqp8XAO7aN"
      },
      "outputs": [],
      "source": [
        "sd = SimpleDiffusion(num_diffusion_timesteps=TrainingConfig.TIMESTEPS, device=\"cpu\")\n",
        "\n",
        "loader = iter(  # converting dataloader into an iterator for now.\n",
        "    get_dataloader(\n",
        "        dataset_name=BaseConfig.DATASET,\n",
        "        batch_size=6,\n",
        "        device=\"cpu\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7lDSGTmtPAO8",
        "outputId": "a5744aa4-d4e8-407a-b9c3-55bd202c0d3e"
      },
      "outputs": [],
      "source": [
        "x0s, _ = next(loader)\n",
        "\n",
        "noisy_images = []\n",
        "specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n",
        "\n",
        "for timestep in specific_timesteps:\n",
        "    timestep = torch.as_tensor(timestep, dtype=torch.long)\n",
        "\n",
        "    xts, _ = forward_diffusion(sd, x0s, timestep)\n",
        "    xts = inverse_transform(xts) / 255.0\n",
        "    xts = make_grid(xts, nrow=1, padding=1)\n",
        "\n",
        "    noisy_images.append(xts)\n",
        "\n",
        "# Plot and see samples at different timesteps\n",
        "\n",
        "_, ax = plt.subplots(1, len(noisy_images), figsize=(20, 10), facecolor=\"white\")\n",
        "\n",
        "for i, (timestep, noisy_sample) in enumerate(zip(specific_timesteps, noisy_images)):\n",
        "    ax[i].imshow(noisy_sample.squeeze(0).permute(1, 2, 0))\n",
        "    ax[i].set_title(f\"t={timestep}\", fontsize=8)\n",
        "    ax[i].axis(\"off\")\n",
        "    ax[i].grid(False)\n",
        "\n",
        "plt.suptitle(\"Forward Diffusion Process\", y=0.9)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh5b7gY0GTNr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0y3eNd0PPC2t"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    BASE_CH = 64  # 64, 128, 256, 256\n",
        "    BASE_CH_MULT = (1, 2, 4, 4) # 32, 16, 8, 8\n",
        "    APPLY_ATTENTION = (False, True, True, False)\n",
        "    DROPOUT_RATE = 0.1\n",
        "    TIME_EMB_MULT = 4 # 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "77P5H4IUPMeW",
        "outputId": "eb1a984b-6b27-476f-b7ac-c0f271b5c9fd"
      },
      "outputs": [],
      "source": [
        "model = UNet(\n",
        "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
        "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
        "    base_channels           = ModelConfig.BASE_CH,\n",
        "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
        "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
        "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
        "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
        ")\n",
        "# print(model)\n",
        "#replace path to stored checkpoint file\n",
        "#checkpoint_dir = '/path'\n",
        "#model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt.tar\"), map_location='cpu')['model'])\n",
        "model.to(BaseConfig.DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig.LR)\n",
        "\n",
        "dataloader = get_dataloader(\n",
        "    dataset_name  = BaseConfig.DATASET,\n",
        "    batch_size    = TrainingConfig.BATCH_SIZE,\n",
        "    device        = BaseConfig.DEVICE,\n",
        "    pin_memory    = True,\n",
        "    num_workers   = TrainingConfig.NUM_WORKERS,\n",
        ")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "sd = SimpleDiffusion(\n",
        "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
        "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
        "    device                  = BaseConfig.DEVICE,\n",
        ")\n",
        "\n",
        "scaler = amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r92CQK6pPUUr",
        "outputId": "fc69aa61-3b34-4246-a62a-682c61388c98"
      },
      "outputs": [],
      "source": [
        "total_epochs = TrainingConfig.NUM_EPOCHS + 1\n",
        "log_dir, checkpoint_dir = setup_log_directory(config=BaseConfig())\n",
        "\n",
        "generate_video = False\n",
        "ext = \".mp4\" if generate_video else \".png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Osaj-IVPPcu3"
      },
      "outputs": [],
      "source": [
        "# Algorithm 1: Training\n",
        "\n",
        "def train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800,\n",
        "                   base_config=BaseConfig(), training_config=TrainingConfig()):\n",
        "\n",
        "    loss_record = MeanMetric()\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n",
        "        tq.set_description(f\"Train :: Epoch: {epoch}/{training_config.NUM_EPOCHS}\")\n",
        "\n",
        "        for x0s, _ in loader:\n",
        "            tq.update(1)\n",
        "\n",
        "            ts = torch.randint(low=1, high=training_config.TIMESTEPS, size=(x0s.shape[0],), device=base_config.DEVICE)\n",
        "            xts, gt_noise = forward_diffusion(sd, x0s, ts)\n",
        "\n",
        "            with amp.autocast(dtype=torch.float16):\n",
        "                pred_noise = model(xts, ts)\n",
        "                loss = loss_fn(gt_noise, pred_noise)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            loss_value = loss.detach().item()\n",
        "            loss_record.update(loss_value)\n",
        "\n",
        "            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n",
        "\n",
        "        mean_loss = loss_record.compute().item()\n",
        "\n",
        "        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n",
        "\n",
        "    return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fIH6ETCu9U3t"
      },
      "outputs": [],
      "source": [
        "# Algorithm 2: Sampling - save each image\n",
        "\n",
        "@torch.no_grad()\n",
        "def reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64),\n",
        "                      num_images=5, nrow=8, device=\"cpu\",save_indv_folder=False, **kwargs):\n",
        "\n",
        "    x = torch.randn((num_images, *img_shape), device=device)\n",
        "    model.eval()\n",
        "\n",
        "    if kwargs.get(\"generate_video\", False):\n",
        "        outs = []\n",
        "\n",
        "    for time_step in tqdm(iterable=reversed(range(1, timesteps)),\n",
        "                          total=timesteps-1, dynamic_ncols=False,\n",
        "                          desc=\"Sampling :: \", position=0):\n",
        "\n",
        "        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n",
        "        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n",
        "\n",
        "        predicted_noise = model(x, ts)\n",
        "\n",
        "        beta_t                            = get(sd.beta, ts)\n",
        "        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n",
        "        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts)\n",
        "\n",
        "        x = (\n",
        "            one_by_sqrt_alpha_t\n",
        "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
        "            + torch.sqrt(beta_t) * z\n",
        "        )\n",
        "\n",
        "        if kwargs.get(\"generate_video\", False):\n",
        "            x_inv = inverse_transform(x).type(torch.uint8)\n",
        "            grid = make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
        "            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n",
        "            outs.append(ndarr)\n",
        "\n",
        "    if kwargs.get(\"generate_video\", False): # Generate and save video of the entire reverse process.\n",
        "        frames2vid(outs, kwargs['save_path'])\n",
        "        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n",
        "        return None\n",
        "\n",
        "    else: # Display and save the image at the final timestep of the reverse process.\n",
        "        x = inverse_transform(x).type(torch.uint8)\n",
        "\n",
        "        # Create a folder for individual images if it doesn't exist\n",
        "        # Initializing timestamp to save folder according to time\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        if (save_indv_folder == True):\n",
        "\n",
        "            individual_images_folder = f'B3_generated_images_193/individual_images_{timestamp}'\n",
        "            os.makedirs(individual_images_folder, exist_ok=True)\n",
        "        else:\n",
        "\n",
        "            individual_images_folder = f'individual_images_training_193/individual_images_{timestamp}'\n",
        "            os.makedirs(individual_images_folder, exist_ok=True)\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            # Save each individual image\n",
        "            individual_image_path = os.path.join(individual_images_folder, f'image_{i+1}.png')\n",
        "            #individual_image = inverse_transform(x[i]).type(torch.uint8)\n",
        "            TF.functional.to_pil_image(x[i]).save(individual_image_path)\n",
        "\n",
        "\n",
        "        grid = make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
        "        pil_image = TF.functional.to_pil_image(grid)\n",
        "        pil_image.save(kwargs['save_path'], format=save_path[-3:].upper())\n",
        "        display(pil_image)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AMgiMxD6TbEJ",
        "outputId": "654acdd8-dcf2-42bc-8233-9c1a959ddabb"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, total_epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Algorithm 1: Training\n",
        "    loss = train_one_epoch(model, sd, dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        save_path = os.path.join(log_dir, f\"{epoch}{ext}\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        # Profiling memory usage\n",
        "        current_memory = memory_allocated()\n",
        "        max_memory = torch.cuda.max_memory_allocated()\n",
        "        cached_memory = memory_cached()\n",
        "\n",
        "        # Algorithm 2: Sampling\n",
        "        reverse_diffusion(model, sd, timesteps=TrainingConfig.TIMESTEPS, num_images=8, generate_video=generate_video,\n",
        "            save_path=save_path, img_shape=TrainingConfig.IMG_SHAPE, device=BaseConfig.DEVICE,\n",
        "        )\n",
        "        # print(f\"Epoch {epoch + 1}:\")\n",
        "        # print(f\"Current memory allocated: {current_memory / 1024 / 1024:.2f} MB\")\n",
        "        # print(f\"Max memory allocated: {max_memory / 1024 / 1024:.2f} MB\")\n",
        "        # print(f\"Cached memory: {cached_memory / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "        # clear_output()\n",
        "        checkpoint_dict = {\n",
        "            \"opt\": optimizer.state_dict(),\n",
        "            \"scaler\": scaler.state_dict(),\n",
        "            \"model\": model.state_dict()\n",
        "        }\n",
        "        torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"ckpt.tar\"))\n",
        "        del checkpoint_dict\n",
        "        # Reset max memory allocated for the next epoch\n",
        "        reset_max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FWhPKL8BPz4A"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1oh7fgxP7lx"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "50chAvNuQShv"
      },
      "outputs": [],
      "source": [
        "model = UNet(\n",
        "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
        "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
        "    base_channels           = ModelConfig.BASE_CH,\n",
        "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
        "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
        "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
        "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
        ")\n",
        "# replace path below\n",
        "checkpoint_dir = '/path'\n",
        "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt_370.tar\"), map_location='cpu')['model']) # weight file\n",
        "#model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt.tar\"), map_location='cpu')['model'])\n",
        "\n",
        "model.to(BaseConfig.DEVICE)\n",
        "\n",
        "sd = SimpleDiffusion(\n",
        "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
        "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
        "    device                  = BaseConfig.DEVICE,\n",
        ")\n",
        "\n",
        "log_dir = \"inference_results\"\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ikPkm99vQAOZ"
      },
      "outputs": [],
      "source": [
        "generate_video = True\n",
        "\n",
        "ext = \".mp4\" if generate_video else \".png\"\n",
        "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
        "\n",
        "save_path = os.path.join(log_dir, filename)\n",
        "\n",
        "reverse_diffusion(\n",
        "    model,\n",
        "    sd,\n",
        "    num_images=6,\n",
        "    generate_video=generate_video,\n",
        "    save_path=save_path,\n",
        "    timesteps=1000,\n",
        "    img_shape=TrainingConfig.IMG_SHAPE,\n",
        "    device=BaseConfig.DEVICE,\n",
        "    nrow=32,\n",
        ")\n",
        "print(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ma-7XkiiQCGl"
      },
      "outputs": [],
      "source": [
        "for i in range(30):\n",
        "    generate_video = False\n",
        "    print(\"epoch_no: \",i)\n",
        "    ext = \".mp4\" if generate_video else \".png\"\n",
        "    filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
        "\n",
        "    save_path = os.path.join(log_dir, filename)\n",
        "\n",
        "\n",
        "    reverse_diffusion(\n",
        "        model,\n",
        "        sd,\n",
        "        num_images=16,\n",
        "        generate_video=generate_video,\n",
        "        save_path=save_path,\n",
        "        timesteps=1000,\n",
        "        img_shape=TrainingConfig.IMG_SHAPE,\n",
        "        device=BaseConfig.DEVICE,\n",
        "        nrow=8,\n",
        "        save_indv_folder = True\n",
        "\n",
        "    )\n",
        "\n",
        "    print(save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
